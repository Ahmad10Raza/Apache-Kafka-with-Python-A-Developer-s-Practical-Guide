{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cffb342",
   "metadata": {},
   "source": [
    "# Kafka Message Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c7cb12",
   "metadata": {},
   "source": [
    "### üì¶ Kafka Message Format ‚Äì Explained in Detail\n",
    "\n",
    "A **Kafka message** is a data unit that is sent from a **producer** to a **topic**, and read by a **consumer**. Kafka stores and transports **binary messages**, but under the hood, each message includes metadata and structure.\n",
    "\n",
    "\n",
    "\n",
    "## üß± Kafka Message Structure\n",
    "\n",
    "Each Kafka message contains the following parts:\n",
    "\n",
    "| Field         | Description                                                                     |\n",
    "| ------------- | ------------------------------------------------------------------------------- |\n",
    "| **Key**       | (Optional) Used for message routing to partitions.                              |\n",
    "| **Value**     | The actual data (e.g., JSON, text, binary). This is what you typically process. |\n",
    "| **Offset**    | Unique ID of the message **within a partition**.                                |\n",
    "| **Partition** | Partition number where the message is stored.                                   |\n",
    "| **Timestamp** | When the message was produced.                                                  |\n",
    "| **Headers**   | (Optional) Key-value pairs for metadata (like HTTP headers).                    |\n",
    "\n",
    "\n",
    "\n",
    "### üìå Example (JSON Message)\n",
    "\n",
    "Here‚Äôs a simple message as seen by a **consumer** (after deserialization):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"sensor_id\": \"sensor_1\",\n",
    "  \"temperature\": 29.3,\n",
    "  \"timestamp\": \"2025-06-02T20:12:11\"\n",
    "}\n",
    "```\n",
    "\n",
    "But under the hood, Kafka stores it like:\n",
    "\n",
    "```text\n",
    "Partition: 0\n",
    "Offset: 12\n",
    "Key: \"sensor_1\"\n",
    "Value: {\"sensor_id\":\"sensor_1\",\"temperature\":29.3,\"timestamp\":\"2025-06-02T20:12:11\"}\n",
    "Timestamp: 2025-06-02 20:12:11\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## üîë Field Breakdown\n",
    "\n",
    "### 1. **Key (Optional)**\n",
    "\n",
    "* Helps Kafka **route** messages to specific **partitions**.\n",
    "* Messages with the same key always go to the **same partition**.\n",
    "* Useful in scenarios like sending all messages of a specific user/device to one partition.\n",
    "\n",
    "### 2. **Value**\n",
    "\n",
    "* The main content (your actual data).\n",
    "* Can be in any format: text, JSON, XML, Avro, binary.\n",
    "* Must be **serialized** into bytes before sending.\n",
    "\n",
    "### 3. **Offset**\n",
    "\n",
    "* A unique identifier **per partition** (not global).\n",
    "* Consumers use this to keep track of where they left off.\n",
    "\n",
    "### 4. **Partition**\n",
    "\n",
    "* Kafka divides a topic into partitions.\n",
    "* Partitions allow **parallelism** and **scaling**.\n",
    "\n",
    "### 5. **Timestamp**\n",
    "\n",
    "* Kafka automatically adds a timestamp to each message.\n",
    "* Can be **producer-generated** or **broker-generated**.\n",
    "\n",
    "### 6. **Headers** (Optional)\n",
    "\n",
    "* Metadata for the message.\n",
    "* Format: `{\"source\": \"sensorA\", \"location\": \"room1\"}`\n",
    "\n",
    "\n",
    "\n",
    "## üìå Real Example (Python Kafka Producer)\n",
    "\n",
    "```python\n",
    "producer.send(\n",
    "    topic='temperature-readings',\n",
    "    key=b'sensor_1',  # key as bytes\n",
    "    value=json.dumps(data).encode('utf-8'),  # value as serialized JSON\n",
    "    headers=[('source', b'raspberry-pi')]\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## üß† Summary\n",
    "\n",
    "| Field     | Purpose                             | Required? |\n",
    "| --------- | ----------------------------------- | --------- |\n",
    "| Key       | Routing to partition                | No        |\n",
    "| Value     | Actual message content              | Yes       |\n",
    "| Offset    | Unique ID in a partition (auto-set) | Auto      |\n",
    "| Partition | Data sharding and parallelism       | Auto/Set  |\n",
    "| Timestamp | Track event time                    | Auto      |\n",
    "| Headers   | Extra metadata                      | No        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e1078",
   "metadata": {},
   "source": [
    "# Offsets and Commit Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7083cdc1",
   "metadata": {},
   "source": [
    "### üß≠ Kafka Offsets and Commit Mechanism ‚Äì Explained Clearly\n",
    "\n",
    "\n",
    "\n",
    "## üî¢ What is an **Offset** in Kafka?\n",
    "\n",
    "An **offset** is a **unique identifier for each message** in a Kafka **partition**.\n",
    "\n",
    "* It's a **sequential number**: `0, 1, 2, 3...`\n",
    "* It **identifies the position** of a message within a **partition**, not across the entire topic.\n",
    "\n",
    "üß† Think of it as a **line number** in a file.\n",
    "If a consumer reads offset `5`, it has processed the **6th message** in that partition.\n",
    "\n",
    "\n",
    "\n",
    "## üß™ Example:\n",
    "\n",
    "| Partition | Message (Value) | Offset |\n",
    "| --------- | --------------- | ------ |\n",
    "| 0         | {\"temp\": 25}    | 0      |\n",
    "| 0         | {\"temp\": 26}    | 1      |\n",
    "| 0         | {\"temp\": 27}    | 2      |\n",
    "\n",
    "If your consumer reads up to offset `1`, it has read the first two messages.\n",
    "\n",
    "\n",
    "\n",
    "## üîÅ What is Offset Commit?\n",
    "\n",
    "Kafka **does not delete messages immediately after consumption**.\n",
    "Instead, consumers **keep track of their own position** using the **commit mechanism**.\n",
    "\n",
    "### ‚úÖ Committing an Offset:\n",
    "\n",
    "* It means ‚Äúüìå I've successfully processed this message. Remember this offset.‚Äù\n",
    "* On restart, the consumer resumes **from the next offset** (not re-reading old data).\n",
    "\n",
    "\n",
    "\n",
    "## üîß Types of Offset Management\n",
    "\n",
    "| Type                           | Description                                                                      |\n",
    "| ------------------------------ | -------------------------------------------------------------------------------- |\n",
    "| **Automatic Commit** (default) | Kafka client **periodically commits** offset in the background (e.g., every 5s). |\n",
    "| **Manual Commit**              | You choose **when to commit** ‚Äî typically after successful processing.           |\n",
    "\n",
    "\n",
    "\n",
    "## üì¶ Where Are Offsets Stored?\n",
    "\n",
    "Kafka stores committed offsets in an internal topic:\n",
    "\n",
    "```\n",
    "__consumer_offsets\n",
    "```\n",
    "\n",
    "Each consumer group maintains its own offset per partition.\n",
    "\n",
    "\n",
    "\n",
    "## üõ†Ô∏è Python Example ‚Äì Auto vs Manual\n",
    "\n",
    "### ‚úÖ Auto Commit (default)\n",
    "\n",
    "```python\n",
    "KafkaConsumer(\n",
    "    'topic-name',\n",
    "    group_id='my-group',\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    enable_auto_commit=True,             # Auto commit is ON\n",
    "    auto_commit_interval_ms=5000         # Commit every 5 seconds\n",
    ")\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è Risk: If app crashes before auto-commit ‚Üí **messages may be reprocessed**.\n",
    "\n",
    "\n",
    "\n",
    "### ‚úÖ Manual Commit\n",
    "\n",
    "```python\n",
    "consumer = KafkaConsumer(\n",
    "    'topic-name',\n",
    "    group_id='my-group',\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    enable_auto_commit=False             # Turn off auto-commit\n",
    ")\n",
    "\n",
    "for message in consumer:\n",
    "    print(message.value)\n",
    "    # Only commit after successful processing\n",
    "    consumer.commit()\n",
    "```\n",
    "\n",
    "This ensures **no message is marked as ‚Äúdone‚Äù** until your code explicitly says so.\n",
    "\n",
    "\n",
    "\n",
    "## üß† Real-World Tip\n",
    "\n",
    "| Scenario                 | Use...                              |\n",
    "| ------------------------ | ----------------------------------- |\n",
    "| Simple logging           | Auto commit                         |\n",
    "| Critical data processing | Manual commit                       |\n",
    "| Long batch jobs          | Manual commit                       |\n",
    "| Exactly-once semantics   | Manual + transaction API (advanced) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868faf73",
   "metadata": {},
   "source": [
    "# Sync vs Async Producer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a0104f",
   "metadata": {},
   "source": [
    "### üöÄ Kafka Sync vs Async Producer ‚Äì Explained Clearly\n",
    "\n",
    "When using a Kafka **Producer** in Python (or any language), you can send messages in two ways:\n",
    "\n",
    "\n",
    "\n",
    "## üîÅ 1. **Synchronous Producer**\n",
    "\n",
    "### ‚úÖ What is it?\n",
    "\n",
    "* The producer sends a message and **waits** for Kafka to **acknowledge** it before continuing.\n",
    "* It's **blocking**: the next line runs only after Kafka confirms success/failure.\n",
    "\n",
    "### üì¶ Example (Python):\n",
    "\n",
    "```python\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "future = producer.send('test-topic', {\"name\": \"sync message\"})\n",
    "result = future.get(timeout=10)  # blocks until acknowledged\n",
    "\n",
    "print(\"Message sent:\", result)\n",
    "```\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "\n",
    "* Reliable\n",
    "* Easy to debug\n",
    "\n",
    "### ‚ùå Cons:\n",
    "\n",
    "* Slower (waits after each message)\n",
    "* Not suitable for high-throughput systems\n",
    "\n",
    "\n",
    "\n",
    "## ‚ö° 2. **Asynchronous Producer**\n",
    "\n",
    "### ‚úÖ What is it?\n",
    "\n",
    "* The producer sends messages and **doesn't wait** for the response.\n",
    "* Uses **callback functions** to handle success or failure.\n",
    "* It's **non-blocking**.\n",
    "\n",
    "### üì¶ Example (Python):\n",
    "\n",
    "```python\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "def on_success(record_metadata):\n",
    "    print(\"Message sent to:\", record_metadata.topic, record_metadata.partition)\n",
    "\n",
    "def on_error(excp):\n",
    "    print(\"Failed to send message:\", excp)\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "producer.send('test-topic', {\"name\": \"async message\"}).add_callback(on_success).add_errback(on_error)\n",
    "\n",
    "# optional: wait for all async messages to be delivered\n",
    "producer.flush()\n",
    "```\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "\n",
    "* High performance\n",
    "* Ideal for streaming large volumes of data\n",
    "\n",
    "### ‚ùå Cons:\n",
    "\n",
    "* More complex (callbacks)\n",
    "* Need careful error handling\n",
    "\n",
    "\n",
    "\n",
    "## ‚öñÔ∏è Summary: Sync vs Async\n",
    "\n",
    "| Feature        | Synchronous                   | Asynchronous                       |\n",
    "| -------------- | ----------------------------- | ---------------------------------- |\n",
    "| Blocking       | Yes                           | No                                 |\n",
    "| Speed          | Slower                        | Faster                             |\n",
    "| Error Handling | Straightforward (try/except)  | Via callbacks (success/error)      |\n",
    "| Use Case       | Simpler, low-throughput tasks | High-throughput, real-time systems |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48828c2",
   "metadata": {},
   "source": [
    "# Auto Commit vs Manual Commit (Consumer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6f9d98",
   "metadata": {},
   "source": [
    "### üîÑ Kafka Auto Commit vs Manual Commit in Consumer ‚Äì Explained Clearly\n",
    "\n",
    "Kafka consumers **track what messages they've read** using **offsets**.\n",
    "How and **when** these offsets are saved (\"committed\") to Kafka is controlled by:\n",
    "\n",
    "> ‚úÖ **Auto Commit** or üîß **Manual Commit**\n",
    "\n",
    "\n",
    "\n",
    "## üß≠ What is \"Commit\" in Kafka?\n",
    "\n",
    "Committing an offset means:\n",
    "\n",
    "> üìå ‚ÄúHey Kafka, I have processed this message. If I restart, I‚Äôll start from the next one.‚Äù\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ Auto Commit\n",
    "\n",
    "### üìã Behavior:\n",
    "\n",
    "* Kafka **automatically commits offsets** in the background at fixed intervals.\n",
    "\n",
    "### üîß Config:\n",
    "\n",
    "```python\n",
    "KafkaConsumer(\n",
    "    'my-topic',\n",
    "    group_id='my-group',\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    enable_auto_commit=True,         # Turned ON (default)\n",
    "    auto_commit_interval_ms=5000     # Commit every 5 seconds\n",
    ")\n",
    "```\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "\n",
    "* Easy to use\n",
    "* Less code\n",
    "\n",
    "### ‚ùå Cons:\n",
    "\n",
    "* Not always reliable ‚Äî if the consumer crashes before commit, messages may be reprocessed.\n",
    "* You **can‚Äôt guarantee exactly-once** processing.\n",
    "\n",
    "\n",
    "\n",
    "## üîß Manual Commit\n",
    "\n",
    "### üìã Behavior:\n",
    "\n",
    "* You control **when** to commit the offset, typically after successful processing.\n",
    "\n",
    "### üîß Config:\n",
    "\n",
    "```python\n",
    "consumer = KafkaConsumer(\n",
    "    'my-topic',\n",
    "    group_id='my-group',\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    enable_auto_commit=False\n",
    ")\n",
    "\n",
    "for message in consumer:\n",
    "    # Process message\n",
    "    print(message.value)\n",
    "    \n",
    "    # Commit after processing\n",
    "    consumer.commit()\n",
    "```\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "\n",
    "* More control\n",
    "* Helps avoid duplicate processing\n",
    "* Required for **exactly-once or at-least-once** patterns\n",
    "\n",
    "### ‚ùå Cons:\n",
    "\n",
    "* Slightly more complex\n",
    "* You must remember to commit manually\n",
    "\n",
    "\n",
    "\n",
    "## ‚öñÔ∏è Comparison Table\n",
    "\n",
    "| Feature              | Auto Commit                  | Manual Commit                   |\n",
    "| -------------------- | ---------------------------- | ------------------------------- |\n",
    "| Offset commit timing | Periodic, automatic          | Explicit, controlled by dev     |\n",
    "| Risk of duplication  | Yes (if crash before commit) | Low (commit only after success) |\n",
    "| Setup effort         | Minimal                      | More coding required            |\n",
    "| Best for             | Simple/logging consumers     | Critical, transactional logic   |\n",
    "\n",
    "\n",
    "\n",
    "## üß† Real-world Rule of Thumb\n",
    "\n",
    "| Use Case                        | Commit Type   |\n",
    "| ------------------------------- | ------------- |\n",
    "| Logging, analytics, dashboards  | Auto Commit   |\n",
    "| Payments, Orders, Notifications | Manual Commit |\n",
    "| ETL pipelines                   | Manual Commit |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
