{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8700aa8e",
   "metadata": {},
   "source": [
    "\n",
    "### ğŸ§  Roadmap to Learn Apache Kafka with Python\n",
    "\n",
    "\n",
    "\n",
    "#### ğŸ“ **Module 1: Introduction to Apache Kafka**\n",
    "\n",
    "* What is Apache Kafka?\n",
    "* Use Cases (Real-world examples like Uber, Netflix, etc.)\n",
    "* Kafka Architecture (Broker, Producer, Consumer, Topics, Partitions, Zookeeper)\n",
    "* Key Terms: Topic, Partition, Offset, Consumer Group\n",
    "\n",
    "\n",
    "\n",
    "#### ğŸ“ **Module 2: Kafka Setup**\n",
    "\n",
    "* Install Kafka and Zookeeper locally (Windows/Linux/Mac)\n",
    "* Start Kafka Broker and Zookeeper\n",
    "* Kafka CLI: Create Topic, Produce and Consume messages via terminal\n",
    "\n",
    "\n",
    "\n",
    "#### ğŸ“ **Module 3: Kafka with Python (using `confluent-kafka` or `kafka-python`)**\n",
    "\n",
    "* Install Python Kafka client (`pip install kafka-python`)\n",
    "* Write a Kafka **Producer** in Python\n",
    "* Write a Kafka **Consumer** in Python\n",
    "* Serialization & Deserialization (JSON)\n",
    "\n",
    "\n",
    "\n",
    "#### ğŸ“ **Module 4: Kafka Internals**\n",
    "\n",
    "* Kafka Message Format\n",
    "* Offsets and Commit Mechanism\n",
    "* Sync vs Async Producer\n",
    "* Auto Commit vs Manual Commit (Consumer)\n",
    "\n",
    "\n",
    "\n",
    "#### ğŸ“ **Module 5: Real-World Mini Projects**\n",
    "\n",
    "1. **Real-time Log Processor**\n",
    "2. **Order System Simulation**\n",
    "3. **IoT Sensor Data Streamer**\n",
    "\n",
    "\n",
    "\n",
    "#### ğŸ“ **Module 6: Advanced Kafka**\n",
    "\n",
    "* Kafka Streams (basic intro)\n",
    "* Kafka Connect\n",
    "* Schema Registry and Avro\n",
    "* Monitoring Kafka\n",
    "* Kafka Security Basics (SSL, SASL)\n",
    "\n",
    "\n",
    "\n",
    "#### ğŸ“ **Module 7: Deploy Kafka on Cloud / Docker**\n",
    "\n",
    "* Kafka with Docker Compose\n",
    "* Kafka on Confluent Cloud or AWS MSK\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871723eb",
   "metadata": {},
   "source": [
    "# What is Apache Kafka?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ac32c",
   "metadata": {},
   "source": [
    "### ğŸ§  What is Apache Kafka?\n",
    "\n",
    "**Apache Kafka** is a **distributed event streaming platform** used to **build real-time data pipelines and streaming applications**. It allows systems to publish, store, and process streams of data in a **fault-tolerant**, **scalable**, and **high-performance** manner.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ” Kafka in Simple Terms:\n",
    "\n",
    "Imagine Kafka as a **messaging system** â€” like a **postal service**:\n",
    "\n",
    "* You drop a message into a mailbox (Producer).\n",
    "* Kafka stores it temporarily (Broker).\n",
    "* A subscriber opens the mailbox and reads the message (Consumer).\n",
    "\n",
    "\n",
    "\n",
    "### âš™ï¸ Key Capabilities:\n",
    "\n",
    "* **Publish and Subscribe** to streams of records (events).\n",
    "* **Store** streams of data in a fault-tolerant way.\n",
    "* **Process** streams in real time.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ§± Kafka Architecture (Overview):\n",
    "\n",
    "* **Producer** â€“ Sends data to Kafka.\n",
    "* **Topic** â€“ A category to which data is sent (like a channel).\n",
    "* **Partition** â€“ Each topic is split into partitions for scalability.\n",
    "* **Broker** â€“ Kafka server that stores and serves data.\n",
    "* **Consumer** â€“ Reads data from Kafka.\n",
    "* **Zookeeper** â€“ Manages Kafka brokers (deprecated in newer versions).\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”„ Kafka Data Flow:\n",
    "\n",
    "1. A **Producer** sends a message to a **Topic**.\n",
    "2. The Topic stores messages in **Partitions**.\n",
    "3. A **Consumer** subscribes to the Topic and receives messages.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¢ Real-World Use Cases:\n",
    "\n",
    "* **Uber**: Tracking real-time locations of rides.\n",
    "* **Netflix**: Logging and monitoring systems.\n",
    "* **LinkedIn**: Activity feeds and metrics.\n",
    "* **Banks**: Fraud detection and transaction monitoring.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ§‘â€ğŸ’» Example Use Case:\n",
    "\n",
    "A weather station sends temperature readings every second. Kafka collects and streams these readings to different systems for:\n",
    "\n",
    "* Real-time dashboards\n",
    "* Alerts if thresholds are breached\n",
    "* Storing historical data for analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f999648",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ğŸ“¦ Real-World Use Cases of Apache Kafka\n",
    "\n",
    "Apache Kafka powers mission-critical systems in companies across industries by enabling real-time data streaming and event-driven architecture. Here are some major use cases with real-world examples:\n",
    "\n",
    "\n",
    "\n",
    "### ğŸš– 1. **Uber â€“ Real-Time Location and Pricing Updates**\n",
    "\n",
    "* **Use Case**: Kafka streams GPS coordinates and pricing data from drivers and riders.\n",
    "* **How Kafka Helps**: Ensures low-latency, fault-tolerant delivery of location data to services that handle ride matching, surge pricing, and ETA estimation.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ“º 2. **Netflix â€“ Monitoring and Recommendations**\n",
    "\n",
    "* **Use Case**: Kafka handles billions of events per day from user interactions (like watching, pausing, searching).\n",
    "* **How Kafka Helps**: Streams this data into systems for real-time monitoring, anomaly detection, and personalized content recommendations.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ’¼ 3. **LinkedIn â€“ Activity Feeds and Metrics Collection**\n",
    "\n",
    "* **Use Case**: Kafka was originally developed at LinkedIn to handle large-scale activity tracking.\n",
    "* **How Kafka Helps**: Captures profile views, messages, and connection requests to power newsfeeds and analytics.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¦ 4. **Banking and Finance â€“ Fraud Detection**\n",
    "\n",
    "* **Use Case**: Kafka processes transactional data streams to detect fraudulent activities in real-time.\n",
    "* **How Kafka Helps**: Enables systems to act on anomalies instantly by integrating with fraud detection engines.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¬ 5. **Walmart â€“ Real-Time Inventory and Analytics**\n",
    "\n",
    "* **Use Case**: Kafka streams point-of-sale data across stores globally to a central system.\n",
    "* **How Kafka Helps**: Helps in dynamic inventory updates, real-time promotions, and demand forecasting.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸŒ 6. **Airbnb â€“ Event Logging and Search Indexing**\n",
    "\n",
    "* **Use Case**: Kafka streams logs and events to Elasticsearch for real-time search and analytics.\n",
    "* **How Kafka Helps**: Allows building responsive UIs and dashboards for search, booking, and user activity.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸšš 7. **Delivery Platforms (Swiggy, Zomato, DoorDash) â€“ Order Tracking**\n",
    "\n",
    "* **Use Case**: Kafka tracks orders and delivery updates in real time.\n",
    "* **How Kafka Helps**: Notifies customers about order status, delays, or estimated delivery time using live data.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¥ 8. **Healthcare â€“ IoT Medical Device Monitoring**\n",
    "\n",
    "* **Use Case**: Kafka streams sensor data from medical devices to hospital monitoring systems.\n",
    "* **How Kafka Helps**: Enables quick alerts for doctors and medical staff when vital signs exceed safe thresholds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92240da",
   "metadata": {},
   "source": [
    "# Kafka Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fae0b",
   "metadata": {},
   "source": [
    "![Arch](https://www.researchgate.net/publication/368281482/figure/fig5/AS:11431281392324409@1745373215138/Kafka-Operational-Architecture-Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0dd585",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ—ï¸ Apache Kafka Architecture Explained (with Real-Life Scenario)\n",
    "\n",
    "Apache Kafkaâ€™s architecture is designed for **high-throughput**, **scalable**, and **fault-tolerant** data streaming. It is based on **Publish-Subscribe** and **Distributed** messaging principles.\n",
    "\n",
    "Letâ€™s understand the core components and how they work together using a real-life scenario:\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¯ Real-Life Analogy: **News Agency and Newspaper Delivery**\n",
    "\n",
    "Imagine a national news agency that distributes news articles to thousands of subscribers across the country.\n",
    "\n",
    "* **Reporters** are writing the news (like **Producers**).\n",
    "* The news is categorized into sections: politics, sports, weather (like **Topics**).\n",
    "* Each news section is printed in parts across different machines (like **Partitions**).\n",
    "* A **News Distribution Center** stores the newspapers (like the **Broker**).\n",
    "* Subscribers (readers) get the newspaper they subscribed to (like **Consumers**).\n",
    "* A central manager coordinates all machines and handles any failures (like **Zookeeper**).\n",
    "\n",
    "Now let's map this analogy to Kafka's architecture:\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”© Kafka Components with Technical + Real-Life Mapping\n",
    "\n",
    "| Kafka Component    | Description                                                 | Real-Life Analogy                                         |\n",
    "| ------------------ | ----------------------------------------------------------- | --------------------------------------------------------- |\n",
    "| **Producer**       | Sends (publishes) data to Kafka topics.                     | Reporters submitting news articles                        |\n",
    "| **Consumer**       | Reads (subscribes to) data from Kafka topics.               | Readers/subscribers reading newspapers                    |\n",
    "| **Topic**          | A category where messages are published.                    | Newspaper sections (e.g., Sports, Politics)               |\n",
    "| **Partition**      | Subdivision of a topic; allows Kafka to scale horizontally. | Printing each newspaper section on multiple machines      |\n",
    "| **Broker**         | Kafka server that stores and serves messages.               | News Distribution Center                                  |\n",
    "| **Consumer Group** | A group of consumers working together to read data.         | A team of editors splitting sections to read faster       |\n",
    "| **Zookeeper**      | Manages and coordinates Kafka brokers.                      | The central manager that oversees machines and operations |\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”„ How Data Flows in Kafka (Step-by-Step):\n",
    "\n",
    "1. **A Producer** sends a message (e.g., \"Temperature: 28Â°C\") to a Kafka **Topic** like `\"iot-sensors\"`.\n",
    "2. The topic is split into multiple **Partitions** (e.g., based on location).\n",
    "3. Kafka **Brokers** store these partitions.\n",
    "4. One or more **Consumers** subscribe to the topic and read messages from each partition.\n",
    "5. **Zookeeper** ensures brokers are alive and maintains cluster metadata.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ” Visual Example: IoT Weather App\n",
    "\n",
    "* **Producer**: IoT weather sensors sending temperature data.\n",
    "* **Topic**: `\"weather-data\"` (includes all sensor data).\n",
    "* **Partitions**: Based on city (e.g., `partition-0: Delhi`, `partition-1: Mumbai`)\n",
    "* **Broker**: Kafka server storing partition data.\n",
    "* **Consumer**: A Python app displaying real-time temperature on dashboards.\n",
    "* **Zookeeper**: Keeps track of which brokers are alive and which one is leader for each partition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d1ed9a",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ—ï¸ Key Terms in Apache Kafka\n",
    "\n",
    "Understanding these core Kafka terms is crucial before diving into code or architecture.\n",
    "\n",
    "\n",
    "\n",
    "### 1ï¸âƒ£ **Topic**\n",
    "\n",
    "> A **Topic** is a named channel or category to which producers send messages and consumers subscribe.\n",
    "\n",
    "#### ğŸ“¦ Example:\n",
    "\n",
    "* A topic called `\"order-events\"` could store events like â€œOrder Placed,â€ â€œOrder Shipped,â€ etc.\n",
    "\n",
    "#### ğŸ“° Real-Life Analogy:\n",
    "\n",
    "Like different **sections in a newspaper** â€” Sports, Politics, Technology. You subscribe only to the sections (topics) you care about.\n",
    "\n",
    "\n",
    "\n",
    "### 2ï¸âƒ£ **Partition**\n",
    "\n",
    "> A **Partition** is a horizontal division of a Kafka topic that allows parallelism and scalability.\n",
    "\n",
    "#### âš™ï¸ Details:\n",
    "\n",
    "* Each topic can have multiple partitions.\n",
    "* Messages in a partition are ordered.\n",
    "* Each partition is stored in one Kafka broker.\n",
    "\n",
    "#### ğŸ“¦ Example:\n",
    "\n",
    "* Topic `\"order-events\"` with 3 partitions might divide data by region:\n",
    "\n",
    "  * `Partition 0`: North Region\n",
    "  * `Partition 1`: South Region\n",
    "  * `Partition 2`: West Region\n",
    "\n",
    "#### ğŸ“° Real-Life Analogy:\n",
    "\n",
    "A newspaper section (like Sports) is printed on multiple machines in parallel, each handling a part of the total pages (data).\n",
    "\n",
    "\n",
    "\n",
    "### 3ï¸âƒ£ **Offset**\n",
    "\n",
    "> An **Offset** is a unique ID assigned to each message in a partition. It acts like a message number.\n",
    "\n",
    "#### ğŸ”¢ Details:\n",
    "\n",
    "* Offset is **per partition**, not per topic.\n",
    "* Kafka does not delete offsets unless retention is configured.\n",
    "* Consumers use offsets to track where they left off.\n",
    "\n",
    "#### ğŸ“¦ Example:\n",
    "\n",
    "* Partition 0:\n",
    "\n",
    "  * Offset 0 â†’ \"Order#123 Placed\"\n",
    "  * Offset 1 â†’ \"Order#124 Shipped\"\n",
    "\n",
    "#### ğŸ“° Real-Life Analogy:\n",
    "\n",
    "Like **page numbers** in a book â€” helps you remember where you left off reading.\n",
    "\n",
    "\n",
    "\n",
    "### 4ï¸âƒ£ **Consumer Group**\n",
    "\n",
    "> A **Consumer Group** is a group of consumers that coordinate to consume messages from a topic **without duplication**.\n",
    "\n",
    "#### ğŸ¤ Details:\n",
    "\n",
    "* Each partition is assigned to only one consumer in a group.\n",
    "* Enables parallel processing of topic data.\n",
    "\n",
    "#### ğŸ“¦ Example:\n",
    "\n",
    "* Topic `\"payment-events\"` has 3 partitions.\n",
    "* If you create **3 consumers** in one consumer group, each will read from one partition.\n",
    "\n",
    "#### ğŸ“° Real-Life Analogy:\n",
    "\n",
    "A **team of readers** where each person reads a different part of the newspaper and shares the summary. No duplication, faster processing.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ” Summary Table:\n",
    "\n",
    "| Term               | Description                           | Analogy                          |\n",
    "| ------------------ | ------------------------------------- | -------------------------------- |\n",
    "| **Topic**          | A category to send/read messages from | Newspaper Section                |\n",
    "| **Partition**      | A sub-part of a topic for scalability | Print machines per section       |\n",
    "| **Offset**         | Unique ID per message in a partition  | Page number in a book            |\n",
    "| **Consumer Group** | A team of consumers sharing the load  | Team of readers summarizing news |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
