{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfb6aa2",
   "metadata": {},
   "source": [
    "#  Kafka Streams (basic intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd39ae5",
   "metadata": {},
   "source": [
    "### 📘 Kafka Streams – Basic Introduction\n",
    "\n",
    "**Kafka Streams** is a powerful Java library provided by Apache Kafka to build **real-time stream processing applications** on top of Kafka topics.\n",
    "\n",
    "\n",
    "\n",
    "### 🔹 What Is Kafka Streams?\n",
    "\n",
    "Kafka Streams allows you to:\n",
    "\n",
    "* **Read** data from Kafka topics\n",
    "* **Process/transform/analyze** that data\n",
    "* **Write** the results back to Kafka (or other systems)\n",
    "\n",
    "> It's built **on top of Kafka** itself—so no need for extra cluster components (unlike Apache Flink, Spark, etc.).\n",
    "\n",
    "\n",
    "\n",
    "### 🔧 Key Features\n",
    "\n",
    "| Feature                            | Description                                                              |\n",
    "| ---------------------------------- | ------------------------------------------------------------------------ |\n",
    "| **No cluster required**            | Runs inside a normal Java application – no Spark or Flink cluster needed |\n",
    "| **Scalable**                       | Easily scales by increasing app instances                                |\n",
    "| **Fault-tolerant**                 | Built-in stateful recovery using Kafka changelog topics                  |\n",
    "| **Exactly-once semantics**         | Supported for reliable data processing                                   |\n",
    "| **Windowing, Joins, Aggregations** | Built-in support for complex operations                                  |\n",
    "\n",
    "\n",
    "\n",
    "### 📊 Real-World Use Case\n",
    "\n",
    "**Example: Fraud Detection in a Bank**\n",
    "\n",
    "1. Read **transaction events** from topic `transactions`\n",
    "2. Group by user/account ID\n",
    "3. Apply logic: e.g., more than 5 transactions in <10 seconds → suspicious\n",
    "4. Emit alert into a topic `fraud_alerts`\n",
    "\n",
    "This is a **Kafka Streams** application: reading from Kafka, analyzing in real-time, writing back to Kafka.\n",
    "\n",
    "\n",
    "\n",
    "### 🧱 Core Building Blocks\n",
    "\n",
    "| Component       | Description                                                         |\n",
    "| --------------- | ------------------------------------------------------------------- |\n",
    "| `KStream`       | A stream of events/messages                                         |\n",
    "| `KTable`        | A table abstraction (like materialized views)                       |\n",
    "| `GlobalKTable`  | A broadcasted table accessible to all tasks                         |\n",
    "| `Processor API` | Low-level API for full control over processing                      |\n",
    "| `DSL API`       | High-level API for declarative logic: `map()`, `filter()`, `join()` |\n",
    "\n",
    "\n",
    "\n",
    "### 🔄 Kafka Streams vs Kafka Consumer API\n",
    "\n",
    "| Kafka Consumer API                       | Kafka Streams                            |\n",
    "| ---------------------------------------- | ---------------------------------------- |\n",
    "| Manual offset handling                   | Built-in offset tracking                 |\n",
    "| Single-message handling                  | Supports stateful, windowed processing   |\n",
    "| Needs extra setup for joins, aggregation | Built-in joins, aggregations, windowing  |\n",
    "| Suitable for simple consumers            | Ideal for stream processing applications |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df6953",
   "metadata": {},
   "source": [
    "# Kafka Connect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc05758c",
   "metadata": {},
   "source": [
    "### 🔌 Kafka Connect – Introduction\n",
    "\n",
    "**Kafka Connect** is a **framework** provided by Apache Kafka to **stream data between Kafka and external systems** (databases, cloud storage, key-value stores, file systems, etc.) without writing custom code.\n",
    "\n",
    "\n",
    "\n",
    "### 🚀 Why Kafka Connect?\n",
    "\n",
    "Kafka Connect is designed to:\n",
    "\n",
    "* Simplify **data ingestion** into Kafka\n",
    "* Simplify **data export** from Kafka to other systems\n",
    "* Allow **scalable, fault-tolerant, distributed** data pipelines\n",
    "\n",
    "\n",
    "\n",
    "### 🔁 What Does It Do?\n",
    "\n",
    "Kafka Connect supports **two types of connectors**:\n",
    "\n",
    "| Connector Type       | Purpose                                       | Example               |\n",
    "| -------------------- | --------------------------------------------- | --------------------- |\n",
    "| **Source Connector** | Pulls data from an external system into Kafka | MySQL → Kafka         |\n",
    "| **Sink Connector**   | Pushes data from Kafka to an external system  | Kafka → Elasticsearch |\n",
    "\n",
    "\n",
    "\n",
    "### 🏗️ Architecture Overview\n",
    "\n",
    "Kafka Connect runs in two modes:\n",
    "\n",
    "* **Standalone Mode**: For testing or simple use cases (single machine)\n",
    "* **Distributed Mode**: For production (clustered and scalable)\n",
    "\n",
    "Each Connect instance can run:\n",
    "\n",
    "* **Multiple connectors**\n",
    "* **Multiple tasks** per connector (for parallelism)\n",
    "\n",
    "\n",
    "\n",
    "### ⚙️ Components\n",
    "\n",
    "| Component       | Description                                                   |\n",
    "| --------------- | ------------------------------------------------------------- |\n",
    "| **Connector**   | High-level abstraction, defines where data comes from/goes to |\n",
    "| **Task**        | Actual work unit that does the data movement                  |\n",
    "| **Worker**      | The Kafka Connect process that manages connectors and tasks   |\n",
    "| **Plugin Path** | Directory containing connector JARs                           |\n",
    "\n",
    "\n",
    "\n",
    "### 🧰 Example Use Cases\n",
    "\n",
    "| Use Case                           | Connector                          |\n",
    "| ---------------------------------- | ---------------------------------- |\n",
    "| Stream data from MySQL → Kafka     | MySQL Source Connector             |\n",
    "| Dump Kafka data into Elasticsearch | Elasticsearch Sink Connector       |\n",
    "| Backup Kafka data to AWS S3        | S3 Sink Connector                  |\n",
    "| Monitor changes in Postgres (CDC)  | Debezium Postgres Source Connector |\n",
    "\n",
    "\n",
    "\n",
    "### 📝 Sample Flow\n",
    "\n",
    "Imagine you want to sync real-time updates from **MySQL** to **Kafka**:\n",
    "\n",
    "1. Install MySQL Source Connector (like Debezium)\n",
    "2. Configure JSON file:\n",
    "\n",
    "   ```json\n",
    "   {\n",
    "     \"name\": \"mysql-connector\",\n",
    "     \"config\": {\n",
    "       \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\n",
    "       \"database.hostname\": \"localhost\",\n",
    "       \"database.user\": \"root\",\n",
    "       ...\n",
    "       \"database.server.name\": \"mysql\",\n",
    "       \"database.include.list\": \"salesdb\",\n",
    "       \"table.include.list\": \"salesdb.orders\"\n",
    "     }\n",
    "   }\n",
    "   ```\n",
    "3. Start the connector via REST API:\n",
    "\n",
    "   ```\n",
    "   curl -X POST -H \"Content-Type: application/json\" \\\n",
    "     --data @mysql-source.json \\\n",
    "     http://localhost:8083/connectors\n",
    "   ```\n",
    "\n",
    "Now all changes in the `orders` table stream directly to a Kafka topic.\n",
    "\n",
    "\n",
    "\n",
    "### 📦 Popular Kafka Connectors\n",
    "\n",
    "| Connector     | Type        | Purpose                                 |\n",
    "| ------------- | ----------- | --------------------------------------- |\n",
    "| Debezium      | Source      | CDC from DBs (MySQL, Postgres, MongoDB) |\n",
    "| JDBC          | Source/Sink | Generic database support                |\n",
    "| Elasticsearch | Sink        | Search engine/data analytics            |\n",
    "| Amazon S3     | Sink        | Backup Kafka data                       |\n",
    "| FileStream    | Source/Sink | Read/write from local files             |\n",
    "\n",
    "\n",
    "\n",
    "### 📚 When to Use Kafka Connect?\n",
    "\n",
    "Use Kafka Connect when:\n",
    "\n",
    "* You want **ETL-like pipelines** without building custom code\n",
    "* You need **high-throughput data sync**\n",
    "* You want **out-of-the-box fault tolerance and scalability**\n",
    "* You need to **plug into enterprise systems (DBs, NoSQL, cloud storage, etc.)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9429096",
   "metadata": {},
   "source": [
    "# Schema Registry and Avro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed1563",
   "metadata": {},
   "source": [
    "### 📘 Schema Registry and Avro in Apache Kafka\n",
    "\n",
    "Apache Kafka is often used to exchange structured data between different systems. To ensure **data consistency, versioning, and compatibility**, two important tools are used:\n",
    "\n",
    "\n",
    "\n",
    "## 📦 What is Avro?\n",
    "\n",
    "**Apache Avro** is a **data serialization format** commonly used with Kafka. It allows you to:\n",
    "\n",
    "* Define a **schema** (structure) for your data\n",
    "* Serialize/deserialize data efficiently\n",
    "* Evolve schema over time (add/remove fields)\n",
    "\n",
    "### ✅ Why Avro?\n",
    "\n",
    "* **Compact**: Binary format is much smaller than JSON\n",
    "* **Schema-based**: Helps consumers understand the structure\n",
    "* **Supports schema evolution**: Compatible changes over time\n",
    "\n",
    "### 📄 Example Avro Schema\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"User\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"name\", \"type\": \"string\"},\n",
    "    {\"name\": \"age\", \"type\": \"int\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 🧠 What is Schema Registry?\n",
    "\n",
    "**Confluent Schema Registry** is a separate service that stores and manages **Avro schemas** (also supports Protobuf & JSON Schema).\n",
    "\n",
    "### 🎯 Purpose\n",
    "\n",
    "* Validate messages against their schema before sending\n",
    "* Store **multiple versions** of a schema\n",
    "* Ensure **backward/forward compatibility**\n",
    "* Enable Kafka consumers to **auto-discover schema**\n",
    "\n",
    "\n",
    "\n",
    "## 🧭 How It Works\n",
    "\n",
    "1. **Producer** serializes data using Avro + schema\n",
    "2. **Schema is registered** with Schema Registry and assigned an ID\n",
    "3. Message is sent to Kafka with the schema ID embedded\n",
    "4. **Consumer** reads the message and retrieves the schema using the ID\n",
    "5. Data is deserialized using the correct schema\n",
    "\n",
    "\n",
    "\n",
    "## 🔐 Benefits\n",
    "\n",
    "| Feature              | Benefit                                         |\n",
    "| -------------------- | ----------------------------------------------- |\n",
    "| Central Schema Store | Avoid schema conflicts across teams             |\n",
    "| Versioning           | Update schema without breaking consumers        |\n",
    "| Compatibility Checks | Ensure schema changes won’t break existing apps |\n",
    "| Interoperability     | Allows different teams/languages to work safely |\n",
    "\n",
    "\n",
    "\n",
    "## 🛠 Kafka + Avro + Schema Registry Stack\n",
    "\n",
    "| Component               | Tool                                                         |\n",
    "| ----------------------- | ------------------------------------------------------------ |\n",
    "| Serialization           | Avro                                                         |\n",
    "| Schema Storage          | Confluent Schema Registry                                    |\n",
    "| Kafka Producer/Consumer | `confluent-kafka-python` or `kafka-python` with Avro support |\n",
    "\n",
    "\n",
    "\n",
    "## 📦 Sample Python Producer with Avro\n",
    "\n",
    "Using `confluent_kafka`:\n",
    "\n",
    "```python\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "from confluent_kafka.avro.serializer import SerializerError\n",
    "\n",
    "schema_str = \"\"\"\n",
    "{\n",
    "  \"namespace\": \"example.avro\",\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"User\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"name\", \"type\": \"string\"},\n",
    "    {\"name\": \"age\", \"type\": \"int\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "value_schema = avro.loads(schema_str)\n",
    "\n",
    "producer = AvroProducer(\n",
    "    {\n",
    "        'bootstrap.servers': 'localhost:9092',\n",
    "        'schema.registry.url': 'http://localhost:8081'\n",
    "    },\n",
    "    default_value_schema=value_schema\n",
    ")\n",
    "\n",
    "producer.produce(topic='users', value={\"name\": \"Alice\", \"age\": 30})\n",
    "producer.flush()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 🧪 Avro vs JSON in Kafka\n",
    "\n",
    "| Feature          | Avro | JSON         |\n",
    "| ---------------- | ---- | ------------ |\n",
    "| Compact          | ✅    | ❌ (verbose)  |\n",
    "| Schema Support   | ✅    | ❌ (implicit) |\n",
    "| Schema Evolution | ✅    | ❌            |\n",
    "| Performance      | High | Medium       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d75f60",
   "metadata": {},
   "source": [
    "# Monitoring Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61199733",
   "metadata": {},
   "source": [
    "### 📊 Monitoring Apache Kafka (Beginner to Pro Guide)\n",
    "\n",
    "Monitoring Kafka is essential to ensure it's running efficiently, detect issues early, and scale smoothly. Here’s a detailed breakdown:\n",
    "\n",
    "\n",
    "\n",
    "## 🎯 Why Monitor Kafka?\n",
    "\n",
    "Kafka is a **distributed system**, so failures or performance issues can occur in:\n",
    "\n",
    "* Brokers\n",
    "* Producers\n",
    "* Consumers\n",
    "* Topics/Partitions\n",
    "\n",
    "**Monitoring helps with:**\n",
    "\n",
    "* Latency tracking\n",
    "* Message throughput\n",
    "* Broker health\n",
    "* Consumer lag\n",
    "* Disk usage\n",
    "* Cluster availability\n",
    "\n",
    "\n",
    "\n",
    "## 🔍 What to Monitor?\n",
    "\n",
    "### 1. **Broker Metrics**\n",
    "\n",
    "* **Under-replicated partitions** – Critical for reliability\n",
    "* **Offline partitions** – Data unavailable\n",
    "* **Request rate/latency** – API performance (produce, fetch)\n",
    "* **Disk usage** – Log directory filling up\n",
    "* **Garbage collection (GC)** – JVM performance\n",
    "\n",
    "### 2. **Producer Metrics**\n",
    "\n",
    "* **Record send rate**\n",
    "* **Request latency**\n",
    "* **Error rate** (timeouts, retries)\n",
    "\n",
    "### 3. **Consumer Metrics**\n",
    "\n",
    "* **Consumer lag** – Messages not yet processed\n",
    "* **Throughput** – Messages per second\n",
    "* **Commit rate** – Frequency of offset commits\n",
    "\n",
    "### 4. **Zookeeper Metrics**\n",
    "\n",
    "* **Session count**\n",
    "* **Request latency**\n",
    "* **Ephemeral nodes count**\n",
    "\n",
    "### 5. **Cluster-Level Health**\n",
    "\n",
    "* Number of active brokers\n",
    "* Controller availability\n",
    "* Partition distribution\n",
    "\n",
    "\n",
    "\n",
    "## 🛠 Tools for Kafka Monitoring\n",
    "\n",
    "### 1. **JMX (Java Management Extensions)**\n",
    "\n",
    "Kafka exposes many metrics via JMX. You can connect JMX with monitoring tools like:\n",
    "\n",
    "| Tool                         | Features                             |\n",
    "| ---------------------------- | ------------------------------------ |\n",
    "| **Prometheus + Grafana**     | Open-source, real-time dashboards    |\n",
    "| **Confluent Control Center** | Kafka-native, rich UI, alerts        |\n",
    "| **Datadog**                  | Cloud-based, Kafka plugins           |\n",
    "| **LinkedIn Cruise Control**  | Cluster rebalancing and monitoring   |\n",
    "| **Elastic Stack (ELK)**      | Log analysis with Kafka plugins      |\n",
    "| **Burrow**                   | Specifically tracks **consumer lag** |\n",
    "\n",
    "\n",
    "\n",
    "## 📊 Example: Prometheus + Grafana Monitoring Stack\n",
    "\n",
    "### 1. Kafka Exporter (JMX → Prometheus)\n",
    "\n",
    "Use the [JMX Exporter](https://github.com/prometheus/jmx_exporter) to convert Kafka JMX metrics to Prometheus metrics.\n",
    "\n",
    "```yaml\n",
    "# jmx_exporter.yml (Sample config)\n",
    "rules:\n",
    "  - pattern: \"kafka.server<type=(.+), name=(.+)><>(Count|Value)\"\n",
    "    name: kafka_$1_$2\n",
    "    type: GAUGE\n",
    "```\n",
    "\n",
    "### 2. Start JMX Exporter\n",
    "\n",
    "```bash\n",
    "KAFKA_OPTS=\"-javaagent:/path/jmx_prometheus_javaagent-0.18.0.jar=7071:/path/jmx_exporter.yml\" \\\n",
    "bin/kafka-server-start.sh config/server.properties\n",
    "```\n",
    "\n",
    "### 3. Grafana Dashboard\n",
    "\n",
    "* Use Kafka dashboards from [Grafana.com](https://grafana.com/grafana/dashboards/)\n",
    "* Connect Prometheus as data source\n",
    "\n",
    "\n",
    "\n",
    "## 🚨 Alerting (Recommended)\n",
    "\n",
    "Set up alerts in Grafana/Prometheus or any other tool for:\n",
    "\n",
    "* High consumer lag\n",
    "* Offline brokers\n",
    "* Under-replicated partitions\n",
    "* JVM heap usage > 85%\n",
    "* Disk usage > 90%\n",
    "\n",
    "\n",
    "\n",
    "## ✅ Best Practices\n",
    "\n",
    "| Tip                                            | Why                          |\n",
    "| ---------------------------------------------- | ---------------------------- |\n",
    "| Monitor consumer lag per topic                 | Ensures timely processing    |\n",
    "| Enable log rotation                            | Avoid disk overflow          |\n",
    "| Monitor GC & heap                              | Prevent broker crashes       |\n",
    "| Automate alerts                                | Immediate response to issues |\n",
    "| Track schema errors (if using Schema Registry) | Avoid breaking changes       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c3c44",
   "metadata": {},
   "source": [
    "#  Kafka Security Basics (SSL, SASL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d394f599",
   "metadata": {},
   "source": [
    "### 🔐 Kafka Security Basics: SSL, SASL Made Simple\n",
    "\n",
    "Apache Kafka provides robust security mechanisms to ensure secure data transmission, client authentication, and access control. The core pillars of Kafka security are:\n",
    "\n",
    "\n",
    "\n",
    "## 🛡️ 1. SSL (Secure Sockets Layer)\n",
    "\n",
    "### 🔍 What is SSL?\n",
    "\n",
    "SSL/TLS secures communication between Kafka clients (producers/consumers) and brokers by **encrypting** the data.\n",
    "\n",
    "### 🔐 Features:\n",
    "\n",
    "* Encrypts data-in-transit\n",
    "* Prevents MITM (Man-in-the-Middle) attacks\n",
    "* Can also be used for **client authentication**\n",
    "\n",
    "### 🔧 How to Enable SSL (Broker Side):\n",
    "\n",
    "1. Generate a keystore & truststore:\n",
    "\n",
    "```bash\n",
    "keytool -genkey -keystore kafka.server.keystore.jks -validity 365 -storepass password -keypass password -dname \"CN=localhost\"\n",
    "keytool -export -alias localhost -keystore kafka.server.keystore.jks -file cert-file\n",
    "keytool -import -alias localhost -file cert-file -keystore kafka.server.truststore.jks\n",
    "```\n",
    "\n",
    "2. Update `server.properties`:\n",
    "\n",
    "```properties\n",
    "listeners=SSL://localhost:9093\n",
    "security.inter.broker.protocol=SSL\n",
    "ssl.keystore.location=/path/kafka.server.keystore.jks\n",
    "ssl.keystore.password=password\n",
    "ssl.key.password=password\n",
    "ssl.truststore.location=/path/kafka.server.truststore.jks\n",
    "ssl.truststore.password=password\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 🔐 2. SASL (Simple Authentication and Security Layer)\n",
    "\n",
    "### 🔍 What is SASL?\n",
    "\n",
    "SASL provides **authentication** for Kafka clients using multiple mechanisms:\n",
    "\n",
    "* **PLAIN** – Simple username/password\n",
    "* **SCRAM** – Secure hash-based auth (recommended)\n",
    "* **GSSAPI** – Kerberos-based authentication\n",
    "\n",
    "### 🧪 Example: SASL/PLAIN (Client + Broker)\n",
    "\n",
    "#### 🔧 Broker (`server.properties`)\n",
    "\n",
    "```properties\n",
    "listeners=SASL_PLAINTEXT://localhost:9094\n",
    "security.inter.broker.protocol=SASL_PLAINTEXT\n",
    "sasl.mechanism.inter.broker.protocol=PLAIN\n",
    "sasl.enabled.mechanisms=PLAIN\n",
    "```\n",
    "\n",
    "#### 🔧 JAAS Config (broker)\n",
    "\n",
    "Create `kafka_server_jaas.conf`:\n",
    "\n",
    "```ini\n",
    "KafkaServer {\n",
    "  org.apache.kafka.common.security.plain.PlainLoginModule required\n",
    "  username=\"admin\"\n",
    "  password=\"admin-secret\"\n",
    "  user_admin=\"admin-secret\"\n",
    "  user_user1=\"user1-secret\";\n",
    "};\n",
    "```\n",
    "\n",
    "Run Kafka with:\n",
    "\n",
    "```bash\n",
    "export KAFKA_OPTS=\"-Djava.security.auth.login.config=/path/kafka_server_jaas.conf\"\n",
    "```\n",
    "\n",
    "#### 🔧 Client Properties:\n",
    "\n",
    "```properties\n",
    "security.protocol=SASL_PLAINTEXT\n",
    "sasl.mechanism=PLAIN\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 🔏 3. ACLs (Access Control Lists)\n",
    "\n",
    "You can define **fine-grained permissions** for producers and consumers:\n",
    "\n",
    "```bash\n",
    "bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 \\\n",
    "--add --allow-principal User:producer1 --operation Write --topic my-topic\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 🔐 Best Practices Summary\n",
    "\n",
    "| Feature           | Use For                 | Benefit                       |\n",
    "| ----------------- | ----------------------- | ----------------------------- |\n",
    "| SSL               | Encryption + Auth       | Secure data-in-transit        |\n",
    "| SASL/PLAIN        | Authentication          | Simple user-based auth        |\n",
    "| SASL/SCRAM        | Stronger Auth           | Secure hashed credentials     |\n",
    "| ACLs              | Authorization           | Access control per topic/user |\n",
    "| Kerberos (GSSAPI) | Enterprise environments | Centralized identity mgmt     |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
